{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skyhacks58",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukabombala/skyhacks/blob/master/tasks/main/skyhacks58.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn4C3FbN03f8",
        "colab_type": "code",
        "outputId": "03c0844a-f2d7-4ee5-8cc3-1dcc54313d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UfgOLBFTgrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return 1-f1_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcChK3qlDFzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(y_true, y_pred):\n",
        "    gamma = 2.0\n",
        "    alpha = 0.25\n",
        "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-Vf6byJbTl",
        "colab_type": "code",
        "outputId": "23ee3f79-eeb3-470f-e447-71fb5d4d91a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC1vP-7hVVBc",
        "colab_type": "code",
        "outputId": "5be568ec-48ef-452c-b73b-2be087484205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "base_model_2 = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x2 = base_model_2.output\n",
        "x2 = GlobalAveragePooling2D()(x2)\n",
        "# let's add a fully-connected layer\n",
        "x2 = Dense(1024, activation='relu')(x2)\n",
        "state = Dense(4, activation='softmax')(x2)\n",
        "condition = Dense(4, activation='softmax')(x2)\n",
        "\n",
        "# this is the model we will train\n",
        "model_2 = Model(inputs=base_model_2.input, outputs=[state, condition])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naVtZUskm-Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "stuff = Dense(53, activation='sigmoid')(x)\n",
        "room = Dense(6, activation='softmax')(x)\n",
        "\n",
        "# # this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=[stuff, room])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ5L6aUjcBTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9mgxKensbni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_task_1 = ['Bathroom', 'Bathroom cabinet', 'Bathroom sink', 'Bathtub', 'Bed', 'Bed frame',\n",
        "                 'Bed sheet', 'Bedroom', 'Cabinetry', 'Ceiling', 'Chair', 'Chandelier', 'Chest of drawers',\n",
        "                 'Coffee table', 'Couch', 'Countertop', 'Cupboard', 'Curtain', 'Dining room', 'Door', 'Drawer',\n",
        "                 'Facade', 'Fireplace', 'Floor', 'Furniture', 'Grass', 'Hardwood', 'House', 'Kitchen',\n",
        "                 'Kitchen & dining room table', 'Kitchen stove', 'Living room', 'Mattress', 'Nightstand',\n",
        "                 'Plumbing fixture', 'Property', 'Real estate', 'Refrigerator', 'Roof', 'Room', 'Rural area',\n",
        "                 'Shower', 'Sink', 'Sky', 'Table', 'Tablecloth', 'Tap', 'Tile', 'Toilet', 'Tree', 'Urban area',\n",
        "                 'Wall', 'Window']\n",
        "\n",
        "labels_task2 = ['bathroom', 'bedroom', 'dinning_room', 'house', 'kitchen', 'living_room']\n",
        "\n",
        "labels_task3 = [1,2,3,4]\n",
        "\n",
        "root = os.path.join(os.getcwd(),\"gdrive\",\"My Drive\",\"skyhacks\")\n",
        "rootdir = os.path.join(root, \"resizeduni/\")\n",
        "\n",
        "imgcsv = pd.read_csv(os.path.join(root,\"labels.csv\")) #csv with image info\n",
        "imgcsv2 = imgcsv[imgcsv[\"task2_class\"]!=\"validation\"] #csv with image info without images with \"validation\" room class\n",
        "\n",
        "targets = imgcsv2.drop([\"filename\", \"standard\",\"task2_class\", \"tech_cond\"], axis = 1).to_numpy()\n",
        "targets2 = pd.get_dummies(imgcsv2, columns = [\"task2_class\"])[[\"task2_class_bathroom\",\"task2_class_bedroom\",\n",
        "                                                               \"task2_class_dining_room\",\"task2_class_house\",\"task2_class_kitchen\",\"task2_class_living_room\"]].to_numpy()\n",
        "targets3 = to_categorical(imgcsv.drop([\"filename\", \"task2_class\", \"tech_cond\"]+labels_task_1, axis = 1).to_numpy()-1)\n",
        "targets4 = to_categorical(imgcsv.drop([\"filename\", \"task2_class\", \"standard\"]+labels_task_1, axis = 1).to_numpy()-1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzAxntd1tQW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgset = []\n",
        "\n",
        "for name in imgcsv2[\"filename\"]:\n",
        "    imgset.append(io.imread(os.path.join(rootdir, name))/255)\n",
        "    \n",
        "dataset = np.array([np.array(imgset[i]) for i in range(len(imgset))])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrWbfzY9jh2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgset_2 = []\n",
        "\n",
        "for name in imgcsv[\"filename\"]:\n",
        "    imgset_2.append(io.imread(os.path.join(rootdir, name))/255)\n",
        "    \n",
        "dataset_2 = np.array([np.array(imgset_2[i]) for i in range(len(imgset_2))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ4vfGiMnBAa",
        "colab_type": "code",
        "outputId": "8adac5c7-3cd7-499a-c80f-0856635b34ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset, targets, test_size=0.2, random_state = 42)\n",
        "X_train, X_test, y_train2, y_test2 = train_test_split(dataset, targets2, test_size = 0.2, random_state = 42)\n",
        "\n",
        "EarlyStop = EarlyStopping(monitor= \"val_dense_6_f1\", patience = 5, restore_best_weights=True)\n",
        "\n",
        "losses = {\n",
        "\t\"dense_5\": \"binary_crossentropy\",\n",
        "\t\"dense_6\": \"categorical_crossentropy\",\n",
        "}\n",
        "\n",
        "model.compile(optimizer='adam', loss=losses, metrics = [f1])\n",
        "\n",
        "model.fit(X_train,\n",
        " \t{\"dense_5\": y_train, \"dense_6\": y_train2},\n",
        "\tvalidation_data=(X_test,\n",
        "\t\t{\"dense_5\": y_test, \"dense_6\": y_test2}), epochs=50, callbacks = [EarlyStop], batch_size = 16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1752 samples, validate on 438 samples\n",
            "Epoch 1/50\n",
            "1752/1752 [==============================] - 57s 33ms/step - loss: 1.5976 - dense_5_loss: 0.2541 - dense_6_loss: 1.3435 - dense_5_f1: 0.4628 - dense_6_f1: 0.6167 - val_loss: 0.9690 - val_dense_5_loss: 0.2352 - val_dense_6_loss: 0.7338 - val_dense_5_f1: 0.3621 - val_dense_6_f1: 0.2369\n",
            "Epoch 2/50\n",
            "1752/1752 [==============================] - 37s 21ms/step - loss: 1.1061 - dense_5_loss: 0.2008 - dense_6_loss: 0.9053 - dense_5_f1: 0.3866 - dense_6_f1: 0.3854 - val_loss: 1.2523 - val_dense_5_loss: 0.2350 - val_dense_6_loss: 1.0173 - val_dense_5_f1: 0.2963 - val_dense_6_f1: 0.2609\n",
            "Epoch 3/50\n",
            "1752/1752 [==============================] - 38s 22ms/step - loss: 1.0423 - dense_5_loss: 0.1888 - dense_6_loss: 0.8535 - dense_5_f1: 0.3535 - dense_6_f1: 0.3379 - val_loss: 0.9716 - val_dense_5_loss: 0.2224 - val_dense_6_loss: 0.7491 - val_dense_5_f1: 0.2922 - val_dense_6_f1: 0.2228\n",
            "Epoch 4/50\n",
            "1752/1752 [==============================] - 38s 22ms/step - loss: 0.9561 - dense_5_loss: 0.1810 - dense_6_loss: 0.7750 - dense_5_f1: 0.3434 - dense_6_f1: 0.3064 - val_loss: 1.3950 - val_dense_5_loss: 0.2336 - val_dense_6_loss: 1.1615 - val_dense_5_f1: 0.2932 - val_dense_6_f1: 0.2537\n",
            "Epoch 5/50\n",
            "1752/1752 [==============================] - 37s 21ms/step - loss: 0.9027 - dense_5_loss: 0.1741 - dense_6_loss: 0.7286 - dense_5_f1: 0.3277 - dense_6_f1: 0.2942 - val_loss: 1.4668 - val_dense_5_loss: 0.2265 - val_dense_6_loss: 1.2403 - val_dense_5_f1: 0.2928 - val_dense_6_f1: 0.2631\n",
            "Epoch 6/50\n",
            "1752/1752 [==============================] - 38s 21ms/step - loss: 0.8559 - dense_5_loss: 0.1715 - dense_6_loss: 0.6843 - dense_5_f1: 0.3217 - dense_6_f1: 0.2706 - val_loss: 1.1233 - val_dense_5_loss: 0.2431 - val_dense_6_loss: 0.8802 - val_dense_5_f1: 0.2841 - val_dense_6_f1: 0.2439\n",
            "Epoch 7/50\n",
            "1752/1752 [==============================] - 37s 21ms/step - loss: 0.8559 - dense_5_loss: 0.1696 - dense_6_loss: 0.6863 - dense_5_f1: 0.3174 - dense_6_f1: 0.2616 - val_loss: 1.0805 - val_dense_5_loss: 0.2528 - val_dense_6_loss: 0.8277 - val_dense_5_f1: 0.2950 - val_dense_6_f1: 0.2312\n",
            "Epoch 8/50\n",
            "1752/1752 [==============================] - 37s 21ms/step - loss: 0.8047 - dense_5_loss: 0.1667 - dense_6_loss: 0.6380 - dense_5_f1: 0.3109 - dense_6_f1: 0.2403 - val_loss: 1.4627 - val_dense_5_loss: 0.2486 - val_dense_6_loss: 1.2141 - val_dense_5_f1: 0.3001 - val_dense_6_f1: 0.2849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff64929ce48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn2FRTONajga",
        "colab_type": "code",
        "outputId": "63bd820e-6184-4d31-d868-adfb333e2a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_train_2, X_test_2, y_train3, y_test3 = train_test_split(dataset_2, targets3, test_size=0.2, random_state = 42)\n",
        "X_train_2, X_test_2, y_train4, y_test4 = train_test_split(dataset_2, targets4, test_size = 0.2, random_state = 42)\n",
        "\n",
        "class_weight = {\n",
        "    1: 1,\n",
        "    2: 1,\n",
        "    3: 11,\n",
        "    4: 7\n",
        "}\n",
        "\n",
        "EarlyStop2 = EarlyStopping(monitor= \"val_dense_3_f1\", patience = 6, restore_best_weights=True)\n",
        "\n",
        "losses_2 = {\n",
        "\t\"dense_2\": \"categorical_crossentropy\",\n",
        "\t\"dense_3\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "model_2.compile(optimizer='adam', loss=losses_2, metrics = [f1])\n",
        "\n",
        "model_2.fit(X_train_2,\n",
        "\t{\"dense_2\": y_train3, \"dense_3\": y_train4},\n",
        "\tvalidation_data=(X_test_2,\n",
        "\t\t{\"dense_2\": y_test3, \"dense_3\": y_test4}), epochs=50, callbacks = [EarlyStop2], \n",
        "            batch_size=24, class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 2192 samples, validate on 549 samples\n",
            "Epoch 1/50\n",
            "2192/2192 [==============================] - 193s 88ms/step - loss: 1.4299 - dense_2_loss: 0.4729 - dense_3_loss: 0.9570 - dense_2_f1: 0.1159 - dense_3_f1: 0.4616 - val_loss: 4.6128 - val_dense_2_loss: 1.0275 - val_dense_3_loss: 3.5853 - val_dense_2_f1: 0.1002 - val_dense_3_f1: 0.4198\n",
            "Epoch 2/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 1.1494 - dense_2_loss: 0.3646 - dense_3_loss: 0.7848 - dense_2_f1: 0.1053 - dense_3_f1: 0.3969 - val_loss: 1.2933 - val_dense_2_loss: 0.4533 - val_dense_3_loss: 0.8401 - val_dense_2_f1: 0.1239 - val_dense_3_f1: 0.4210\n",
            "Epoch 3/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 1.0406 - dense_2_loss: 0.3267 - dense_3_loss: 0.7140 - dense_2_f1: 0.1040 - dense_3_f1: 0.3360 - val_loss: 1.4019 - val_dense_2_loss: 0.3685 - val_dense_3_loss: 1.0334 - val_dense_2_f1: 0.1012 - val_dense_3_f1: 0.4621\n",
            "Epoch 4/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.9201 - dense_2_loss: 0.2735 - dense_3_loss: 0.6466 - dense_2_f1: 0.0965 - dense_3_f1: 0.2846 - val_loss: 1.5493 - val_dense_2_loss: 0.5169 - val_dense_3_loss: 1.0324 - val_dense_2_f1: 0.2701 - val_dense_3_f1: 0.4958\n",
            "Epoch 5/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.9417 - dense_2_loss: 0.2861 - dense_3_loss: 0.6555 - dense_2_f1: 0.0955 - dense_3_f1: 0.3031 - val_loss: 7.7247 - val_dense_2_loss: 1.4081 - val_dense_3_loss: 6.3165 - val_dense_2_f1: 0.1002 - val_dense_3_f1: 0.4358\n",
            "Epoch 6/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.9031 - dense_2_loss: 0.2756 - dense_3_loss: 0.6275 - dense_2_f1: 0.0949 - dense_3_f1: 0.2701 - val_loss: 2.6275 - val_dense_2_loss: 0.7057 - val_dense_3_loss: 1.9218 - val_dense_2_f1: 0.1002 - val_dense_3_f1: 0.4605\n",
            "Epoch 7/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.7607 - dense_2_loss: 0.2330 - dense_3_loss: 0.5277 - dense_2_f1: 0.0857 - dense_3_f1: 0.2223 - val_loss: 1.5078 - val_dense_2_loss: 0.3821 - val_dense_3_loss: 1.1257 - val_dense_2_f1: 0.1113 - val_dense_3_f1: 0.4160\n",
            "Epoch 8/50\n",
            "2192/2192 [==============================] - 130s 59ms/step - loss: 0.6512 - dense_2_loss: 0.1894 - dense_3_loss: 0.4617 - dense_2_f1: 0.0684 - dense_3_f1: 0.2018 - val_loss: 1.4223 - val_dense_2_loss: 0.4870 - val_dense_3_loss: 0.9353 - val_dense_2_f1: 0.1902 - val_dense_3_f1: 0.4560\n",
            "Epoch 9/50\n",
            "2192/2192 [==============================] - 130s 59ms/step - loss: 0.5133 - dense_2_loss: 0.1478 - dense_3_loss: 0.3654 - dense_2_f1: 0.0527 - dense_3_f1: 0.1439 - val_loss: 1.6330 - val_dense_2_loss: 0.4206 - val_dense_3_loss: 1.2125 - val_dense_2_f1: 0.1014 - val_dense_3_f1: 0.5001\n",
            "Epoch 10/50\n",
            "2192/2192 [==============================] - 130s 59ms/step - loss: 0.4983 - dense_2_loss: 0.1611 - dense_3_loss: 0.3372 - dense_2_f1: 0.0534 - dense_3_f1: 0.1319 - val_loss: 7.6278 - val_dense_2_loss: 1.3831 - val_dense_3_loss: 6.2447 - val_dense_2_f1: 0.1022 - val_dense_3_f1: 0.5637\n",
            "Epoch 11/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.4094 - dense_2_loss: 0.1194 - dense_3_loss: 0.2900 - dense_2_f1: 0.0431 - dense_3_f1: 0.1108 - val_loss: 8.6362 - val_dense_2_loss: 1.6148 - val_dense_3_loss: 7.0214 - val_dense_2_f1: 0.1002 - val_dense_3_f1: 0.4372\n",
            "Epoch 12/50\n",
            "2192/2192 [==============================] - 130s 59ms/step - loss: 0.3765 - dense_2_loss: 0.1229 - dense_3_loss: 0.2537 - dense_2_f1: 0.0391 - dense_3_f1: 0.0891 - val_loss: 1.9069 - val_dense_2_loss: 0.6432 - val_dense_3_loss: 1.2637 - val_dense_2_f1: 0.1066 - val_dense_3_f1: 0.3578\n",
            "Epoch 13/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.2623 - dense_2_loss: 0.0580 - dense_3_loss: 0.2043 - dense_2_f1: 0.0198 - dense_3_f1: 0.0728 - val_loss: 2.1172 - val_dense_2_loss: 0.8719 - val_dense_3_loss: 1.2453 - val_dense_2_f1: 0.2840 - val_dense_3_f1: 0.4017\n",
            "Epoch 14/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1904 - dense_2_loss: 0.0606 - dense_3_loss: 0.1298 - dense_2_f1: 0.0218 - dense_3_f1: 0.0453 - val_loss: 2.0430 - val_dense_2_loss: 0.6880 - val_dense_3_loss: 1.3550 - val_dense_2_f1: 0.1221 - val_dense_3_f1: 0.3720\n",
            "Epoch 15/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.2136 - dense_2_loss: 0.0490 - dense_3_loss: 0.1646 - dense_2_f1: 0.0177 - dense_3_f1: 0.0635 - val_loss: 2.2985 - val_dense_2_loss: 0.7645 - val_dense_3_loss: 1.5339 - val_dense_2_f1: 0.1012 - val_dense_3_f1: 0.3829\n",
            "Epoch 16/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1533 - dense_2_loss: 0.0526 - dense_3_loss: 0.1008 - dense_2_f1: 0.0181 - dense_3_f1: 0.0368 - val_loss: 3.0171 - val_dense_2_loss: 0.7654 - val_dense_3_loss: 2.2517 - val_dense_2_f1: 0.2310 - val_dense_3_f1: 0.3976\n",
            "Epoch 17/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1302 - dense_2_loss: 0.0407 - dense_3_loss: 0.0895 - dense_2_f1: 0.0156 - dense_3_f1: 0.0270 - val_loss: 2.6631 - val_dense_2_loss: 1.0000 - val_dense_3_loss: 1.6631 - val_dense_2_f1: 0.0984 - val_dense_3_f1: 0.3414\n",
            "Epoch 18/50\n",
            "2192/2192 [==============================] - 128s 59ms/step - loss: 0.1646 - dense_2_loss: 0.0402 - dense_3_loss: 0.1244 - dense_2_f1: 0.0124 - dense_3_f1: 0.0454 - val_loss: 2.9172 - val_dense_2_loss: 1.0945 - val_dense_3_loss: 1.8227 - val_dense_2_f1: 0.0975 - val_dense_3_f1: 0.3602\n",
            "Epoch 19/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1332 - dense_2_loss: 0.0291 - dense_3_loss: 0.1041 - dense_2_f1: 0.0110 - dense_3_f1: 0.0361 - val_loss: 2.0627 - val_dense_2_loss: 0.6461 - val_dense_3_loss: 1.4166 - val_dense_2_f1: 0.1207 - val_dense_3_f1: 0.4581\n",
            "Epoch 20/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1944 - dense_2_loss: 0.0570 - dense_3_loss: 0.1374 - dense_2_f1: 0.0183 - dense_3_f1: 0.0520 - val_loss: 22.0304 - val_dense_2_loss: 15.0056 - val_dense_3_loss: 7.0248 - val_dense_2_f1: 0.9690 - val_dense_3_f1: 0.4379\n",
            "Epoch 21/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.3203 - dense_2_loss: 0.1231 - dense_3_loss: 0.1972 - dense_2_f1: 0.0416 - dense_3_f1: 0.0699 - val_loss: 2.3168 - val_dense_2_loss: 0.5380 - val_dense_3_loss: 1.7788 - val_dense_2_f1: 0.1259 - val_dense_3_f1: 0.4418\n",
            "Epoch 22/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1097 - dense_2_loss: 0.0306 - dense_3_loss: 0.0791 - dense_2_f1: 0.0094 - dense_3_f1: 0.0255 - val_loss: 2.4593 - val_dense_2_loss: 0.7446 - val_dense_3_loss: 1.7148 - val_dense_2_f1: 0.1012 - val_dense_3_f1: 0.3717\n",
            "Epoch 23/50\n",
            "2192/2192 [==============================] - 129s 59ms/step - loss: 0.1095 - dense_2_loss: 0.0400 - dense_3_loss: 0.0695 - dense_2_f1: 0.0153 - dense_3_f1: 0.0236 - val_loss: 2.9175 - val_dense_2_loss: 0.8392 - val_dense_3_loss: 2.0783 - val_dense_2_f1: 0.1085 - val_dense_3_f1: 0.3875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff64f23bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WJEeMNMtyBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = []\n",
        "ans = np.zeros((30, 57), dtype=\"object\")\n",
        "rootdir = os.path.join(root, \"resized_test_dataset/\")\n",
        "for i, name in enumerate(os.listdir(rootdir)):\n",
        "    ans[i][0]=name\n",
        "    testset.append(io.imread(os.path.join(rootdir, name))/255)\n",
        "    \n",
        "testset = np.array([np.array(testset[i]) for i in range(len(testset))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAJsjbG-72fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = model.predict(testset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAnRz4vq77oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y[0])):\n",
        "    for j in range(len(y[0][0])):\n",
        "        ans[i][j+4]= 1 if y[0][i][j] >=0.5 else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiDRSckRG4bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y[1])):\n",
        "  ans[i][2] = labels_task2[np.argmax(y[1][i])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpWrkk8Lnjr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2 = model_2.predict(testset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXoHuS_mnzZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y2[0])):\n",
        "  ans[i][1] = labels_task3[np.argmax(y2[0][i])]\n",
        "  ans[i][3] = labels_task3[np.argmax(y2[1][i])]\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnqI8OyU-tQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(ans,columns=['filename', 'standard', 'task2_class', 'tech_cond'] + labels_task_1)\n",
        "df.to_csv(\"ans.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7vnqHb1_v6N",
        "colab_type": "code",
        "outputId": "0f93833f-2398-4230-f3f9-44225d4551a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlPI-dzb_yOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye5Z2WLYGTnD",
        "colab_type": "code",
        "outputId": "70d53769-1f85-4247-f255-598d73e5b881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "folder = \"test\"\n",
        "resizedfolder = \"test_resized\"\n",
        "\n",
        "for file in os.listdir(folder):\n",
        "  if file[:-3]==\"jpg\":\n",
        "    im = Image.open(os.path.join(folder,file))\n",
        "    im.convert(\"RGB\")\n",
        "    im = im.resize((224,224))\n",
        "    im.save(os.path.join(resizedfolder,file))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-238e93b3c1cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresizedfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_resized\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"jpg\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-nw-AzJHSQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = []\n",
        "ans = np.zeros((30, 57), dtype=\"object\")\n",
        "for i, name in enumerate(os.listdir(resizedfolder)):\n",
        "    ans[i][0]=name\n",
        "    testset.append(io.imread(os.path.join(resizedfolder, name))/255)\n",
        "    \n",
        "testset = np.array([np.array(testset[i]) for i in range(len(testset))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z33RVOFVSEcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = model.predict(testset)\n",
        "for i in range(len(y[0])):\n",
        "    for j in range(len(y[0][0])):\n",
        "        ans[i][j+4]= 1 if y[0][i][j] >=0.5 else 0\n",
        "\n",
        "for i in range(len(y[1])):\n",
        "  ans[i][2] = labels_task2[np.argmax(y[1][i])]\n",
        "\n",
        "y2 = model_2.predict(testset)\n",
        "\n",
        "for i in range(len(y2[0])):\n",
        "  ans[i][1] = labels_task3[np.argmax(y2[0][i])]\n",
        "  ans[i][3] = labels_task3[np.argmax(y2[1][i])]\n",
        "\n",
        "df = pd.DataFrame(ans,columns=['filename', 'standard', 'task2_class', 'tech_cond'] + labels_task_1)\n",
        "df.to_csv(\"finalans.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}